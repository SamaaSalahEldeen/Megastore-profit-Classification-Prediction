megastore prediction overview
This repository contains code for feature selection and regression modeling using Lasso, Mutual Information, Anova, Polynomial Regression, Linear Regression, and Ridge Regression.

Regularization Using Lasso
The Lasso model is instantiated with alpha=0.1 to handle a dataset where there's no linear relationship between the target column and other columns. The feature importance using Lasso is visualized, showcasing the coefficients of the model.

Mutual Information Feature Selection
Mutual information between each feature and the target variable is computed. The top k features are identified and printed. Unwanted columns are then dropped from the dataset.

Anova Feature Selection
Anova feature selection is performed to determine the significance of each feature. The top 15 most important features are printed for both the training and testing sets.

Model Deployment and Evaluation
Polynomial Regression
A polynomial regression model is created using a pipeline with PolynomialFeatures and LinearRegression. Hyperparameters are tuned using GridSearchCV, and the model is evaluated using metrics such as RMSE, MSE, and R-squared. A scatter plot visualizes the model predictions.

Linear Regression
Linear regression is performed with hyperparameter tuning using GridSearchCV. Coefficients, intercept, MSE, and R-squared are printed, and a scatter plot illustrates the model predictions.

Lasso Regression
Lasso regression is implemented with hyperparameter tuning using GridSearchCV. The best hyperparameters, mean squared error, and R-squared are printed. A scatter plot shows the predictions.

Ridge Regression
Ridge regression is carried out with hyperparameter tuning using GridSearchCV. The best hyperparameters, mean squared error, and R-squared are printed. A scatter plot visualizes the predictions.
