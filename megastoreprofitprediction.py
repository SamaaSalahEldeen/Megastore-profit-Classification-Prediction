# -*- coding: utf-8 -*-
"""MegastoreProfitPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mPI5bIXAzvCZN4P1NdEAnpxy6YZNMJ40

#**Megastore Profit Prediction**
Can we predict a shipmentâ€™s profit based on a number of factors such as order categories, order date and address among other factors?. If a megastore could predict the profit on each order before shipment, they can understand which factors affect their revenue and control them as such.

```
# This is formatted as code
```

# Used Modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msnum                             # Draw diagram shows the distribution of the null values
import json
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder,LabelEncoder   # Perform Lable encoding and one-hot encoding to string values
from sklearn.feature_selection import mutual_info_regression
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Lasso
from tabulate import tabulate
from sklearn.feature_selection import f_regression, SelectKBest
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from scipy.stats import norm

"""#**Dataset Analysis and Cleaning**

##Load Dataset
"""

dataset = pd.read_csv("/content/megastore-regression-dataset.csv")
dataset.head()

"""Realation between featuers"""

# Segregation of Numerical and Categorical Variables/Columns
cat_col = dataset.select_dtypes(include=['object']).columns
num_col = dataset.select_dtypes(exclude=['object']).columns
print(cat_col)
print(num_col)

plt.figure(figsize=(20,20))
plt.title('Megastore Distribution Plot')
sns.pairplot(dataset[num_col])
plt.show()

"""Exploratory data analysis (EDA)

"""

plt.figure(figsize=(8,8))
plt.title('Megastore Sales Distribution Plot')
sns.distplot(dataset['Quantity'])
plt.show()

"""##Attributes of dataset"""

uniq_table = []

for key in dataset:
  uniq_values = dataset[key].unique()
  uniq_values.sort()
  dtype = (dataset[key].dtype, "string")[dataset[key].dtype == "O"]
  uniq_table.append([key, dtype, len(uniq_values), uniq_values])

print(tabulate(uniq_table, headers=["Attribute", "Data Type", "Unique Values Count", "Unique Values"], tablefmt="pipe"))

"""# Check if the Data contains Null values"""

msnum.matrix(dataset)
dataset.isnull().sum()
# Replace missing values with mean of each column
dataset.fillna(dataset.mean(), inplace=True)

# recheck if the dataset contains any Null values
dataset.isnull().sum()

"""## Remove Duplicates"""

print("Dataset before removing duplicates: ", dataset.shape)

dataset.drop_duplicates(inplace=True)

print("Dataset after removing duplicates: ", dataset.shape)

"""##Convert Date columns to Datetime format"""

# Convert date columns to datetime format
dataset['Order Date'] = pd.to_datetime(dataset['Order Date'], format='%m/%d/%Y')
dataset['Ship Date'] = pd.to_datetime(dataset['Ship Date'], format='%m/%d/%Y')

"""## Features Enginearing

### Extract a new feature " time to deliver "
"""

dataset['Time to Deliver'] = dataset['Order Date'] - dataset['Ship Date']

"""### Extract Main category and sub-category from the category tree column

"""

# Fix JSON formatting in CategoryTree column
dataset['CategoryTree'] = dataset['CategoryTree'].str.replace("'", '"')

# Convert CategoryTree column from JSON string to dictionary
dataset['CategoryTree'] = dataset['CategoryTree'].apply(lambda x: json.loads(x))

# Extract MainCategory and SubCategory columns
dataset[['MainCategory', 'SubCategory']] = dataset['CategoryTree'].apply(lambda x: pd.Series([x['MainCategory'], x['SubCategory']]))

# Drop the original CategoryTree column if desired
dataset.drop('CategoryTree', axis=1, inplace=True)

"""visualizing the category after extraction"""

df_v=pd.DataFrame(dataset['MainCategory'].value_counts())
plot = df_v.plot.pie(y='MainCategory', figsize=(10, 10))
plt.show()



df_v=pd.DataFrame(dataset['SubCategory'].value_counts())
plot = df_v.plot.pie(y='SubCategory', figsize=(10, 10))
plt.show()


f= plt.figure(figsize=(20,20))
ax=f.add_subplot(3,3,1)
sns.distplot(dataset[(dataset.MainCategory== 'Office Supplies')]["Sales"],color='b',ax=ax)
ax.set_title('Distribution of Sales of Office Supplies');

ax=f.add_subplot(3,3,2)
sns.distplot(dataset[(dataset.MainCategory == 'Furniture')]['Sales'],color='r',ax=ax)
ax.set_title('Distribution of Sales of Furniture');

ax=f.add_subplot(3,3,3)
sns.distplot(dataset[(dataset.MainCategory== 'Technology')]["Sales"],color='orange',ax=ax)
ax.set_title('Distribution of Sales of Technology');
plt.show()

"""##Visualize the relation between the sales and each day of the week"""

# assuming your data is stored in a DataFrame called df
dataset['Order Date'] = pd.to_datetime(dataset['Order Date'])  # convert Order Date column to datetime format
dataset['Day of Week'] = dataset['Order Date'].dt.day_name()  # add a new column with the day of the week
grouped = dataset.groupby('Day of Week')['Sales'].sum()  # group by day of the week and sum up sales

plt.bar(grouped.index, grouped.values)
plt.xlabel('Day of Week')
plt.ylabel('Sales')
plt.show()

"""### Split all date columns to 3 columns " day, month, and year " and Convert to Unix timestamp"""

# Convert 'Order Date' and 'Ship Date' to Unix timestamp
dataset['Order Date'] = (pd.to_datetime(dataset['Order Date']) - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
dataset['Ship Date'] = (pd.to_datetime(dataset['Ship Date']) - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')

# Extract day, month, and year values from delivery date
dataset['Time to Deliver Day'] = dataset['Time to Deliver'].dt.days
dataset['Time to Deliver Month'] = dataset['Time to Deliver'].dt.components['days'] // 30
dataset['Time to Deliver Year'] = dataset['Time to Deliver'].dt.components['days'] // 365

# Split date column into day, month, and year columns
dataset['Order Day'] = pd.to_datetime(dataset['Order Date'], unit='s').dt.day
dataset['Order Month'] = pd.to_datetime(dataset['Order Date'], unit='s').dt.month
dataset['Order Year'] = pd.to_datetime(dataset['Order Date'], unit='s').dt.year

# Drop original date column
dataset.drop('Order Date', axis=1, inplace=True)

# Repeat for 'Ship Date' column
dataset['Ship Day'] = pd.to_datetime(dataset['Ship Date'], unit='s').dt.day
dataset['Ship Month'] = pd.to_datetime(dataset['Ship Date'], unit='s').dt.month
dataset['Ship Year'] = pd.to_datetime(dataset['Ship Date'], unit='s').dt.year
dataset.drop('Ship Date', axis=1, inplace=True)

# Drop the 'Delivery Date' column
dataset.drop('Time to Deliver', axis=1, inplace=True)

"""## Splitting the dataset to X and Y where Y is the target column"""

# Split the dataset to x and y
x = dataset.drop('Profit', axis=1)
y = dataset['Profit']

"""##Calculate Correlation"""

#Get the correlation between the features
megastore_data = dataset.iloc[:, :]
corr = megastore_data.corr()

# print the correlation of each column with the target column
for col in corr.columns:
    print(f"{col} - Profit: {corr.loc[col, 'Profit']}")

# Drop the target column from the correlation matrix
corr = corr.drop('Profit')

# sns.heatmap(dataset.corr(),annot = True)
# plt.show()

"""##Correlation Ploting"""

sns.set(style="white")
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

"""# Features Selection and Data Preprocessing

## Splitting the data to train and test
"""

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""visualizing distribution of mean values  in the train and test set"""

plt.figure(figsize=(16,6))
features = x_train.columns.values[2:202]
plt.title("Distribution of mean values per row in the train and test set")
sns.distplot(x_train[features].mean(axis=1),color="green", kde=True,bins=120, label='train')
sns.distplot(x_test[features].mean(axis=1),color="blue", kde=True,bins=120, label='test')
plt.legend()
plt.show()

"""## Apply Lable encoding to the training and testing data separately"""

# Identify categorical columns
cat_cols = x_train.select_dtypes(include=['object']).columns.tolist()

# Fit label encoder to training and test data
for col in cat_cols:
    label_encoder = LabelEncoder()
    label_encoder.fit(list(x_train[col].values) + list(x_test[col].values))
    x_train[col] = label_encoder.transform(x_train[col])
    x_test[col] = label_encoder.transform(x_test[col])

"""## Make sure all values are float values"""

# Make sure all values' data type are float
x_train = x_train.astype(float)
x_test = x_test.astype(float)

"""Outliers detection and handling using z-score

"""

from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler

# Visualize boxplots of each feature before scaling.
fig, axs = plt.subplots(1, 4, figsize=(15, 5))
axs[0].boxplot(dataset['Sales'])
axs[0].set_title('Sales')
axs[1].boxplot(dataset['Discount'])
axs[1].set_title('Discount')
axs[2].boxplot(dataset['Quantity'])
axs[2].set_title('Quantity')
axs[3].boxplot(dataset['Profit'])
axs[3].set_title('Profit')
plt.show()


# Set the desired significance level (0.05 for a 95% confidence interval).
alpha = 0.05

# Fit the MedianScaler on training data.
scaler = RobustScaler(with_centering=True, with_scaling=False, quantile_range=(25.0, 75.0)).fit(x_train[['Sales']])

# Perform outlier detection and filtering on training data.
train_medians = scaler.transform(x_train[['Sales']])
train_abs_medians = np.abs(train_medians)
train_median_threshold = np.median(train_abs_medians) * norm.ppf(1 - alpha/2)  # Two-tailed test.
train_filtered_entries = (train_abs_medians < train_median_threshold).all(axis=1)
x_train = x_train[train_filtered_entries]
y_train = y_train[train_filtered_entries]

# Perform outlier detection and filtering on testing data.
test_medians = scaler.transform(x_test[['Sales']])
test_abs_medians = np.abs(test_medians)
test_median_threshold = np.median(test_abs_medians) * norm.ppf(1 - alpha/2)  # Two-tailed test.
test_filtered_entries = (test_abs_medians < test_median_threshold).all(axis=1)
x_test = x_test[test_filtered_entries]
y_test = y_test[test_filtered_entries]

#Trying to remove the effect of the outliers on the target variable with min max scaling to scale it from 0 to 1
scaler = MinMaxScaler()

scaler.fit(y_train.values.reshape(-1, 1))

y_train = pd.DataFrame(scaler.transform(y_train.values.reshape(-1, 1)), columns=['Profit'])
y_test = pd.DataFrame(scaler.transform(y_test.values.reshape(-1, 1)), columns=['Profit'])

# Visualize boxplots of each feature after scaling.
fig, axs = plt.subplots(1, 4, figsize=(15, 5))
axs[0].boxplot(x_train['Sales'])
axs[0].set_title('Sales')
axs[1].boxplot(x_train['Discount'])
axs[1].set_title('Discount')
axs[2].boxplot(x_train['Quantity'])
axs[2].set_title('Quantity')
axs[3].boxplot(y_train['Profit'])
axs[3].set_title('Profit')
plt.show()

"""## Calculte correlation"""

# Concatenate the X and y dataframes to get the correlation matrix
train_data = pd.concat([x_train, y_train], axis=1)

# Get the correlation between the features and the target variable
corr = train_data.corr()

# Print the correlation of each feature with the target variable
print("Correlation of each feature with the target variable (Profit):")
print(corr['Profit'])

# Drop the target variable from the correlation matrix
corr = corr.drop('Profit')

"""##Correlation of each column with the target column only"""

# Create a bar plot of the correlations
target_column = 'Profit'
plt.figure(figsize=(10, 5))
corr.plot(kind='bar', width=0.9)
plt.title(f'Correlation of each column with {target_column}')
plt.xlabel('Columns')
plt.ylabel('Correlation')
plt.show()

"""## Regularization Using Lasso as there's no linear relationship between he target column and the other columns"""

# Instantiate the Lasso model with alpha=0.1
lasso = Lasso(alpha=0.1)

# Fit the Lasso model to the training data
lasso.fit(x_train, y_train)

# Get the coefficients of the model
coef = pd.Series(lasso.coef_, index=x.columns)

# Check if X and the number of columns in X match
assert len(coef) == len(x.columns), "Number of columns in X does not match the number of coefficients"

# Sort the coefficients by absolute value
imp_coef = coef.sort_values()

# Plot the coefficients
plt.rcParams['figure.figsize'] = (10.0, 20.0)
imp_coef.plot(kind = "barh")
plt.title("Feature importance using Lasso Model")

"""## Mutual Iformation method"""

# Compute the mutual information between each feature and the target variable
mi = mutual_info_regression(x_train, y_train)

# Get the indices of the top k features
k = 20
top_k_indices = mi.argsort()[::-1][:k]

# Get the names of the top k features
top_k_features = x_train.columns[top_k_indices].tolist()
print('The top', k, 'features are:', top_k_features)

"""## Drop unwanted columns"""

x_train = x_train.drop(['Product Name', 'Customer Name','Order ID'], axis=1)
x_test = x_test.drop(['Product Name', 'Customer Name','Order ID'], axis=1)

"""## Anova Feature selection"""

# Perform mutual information regression on the training set to determine the significance of each feature
selector = SelectKBest(lambda X, y: mutual_info_regression(X, y, discrete_features='auto'), k=15)
x_train_selected = selector.fit_transform(x_train, y_train)

# Get the feature scores and corresponding names from the selector object
feature_scores = selector.scores_
feature_names = x_train.columns.values

# Sort the feature scores and corresponding names in descending order
sorted_indices = feature_scores.argsort()[::-1]
sorted_feature_names = feature_names[sorted_indices]

# Get the top 15 most important features in descending order
top_feature_names = sorted_feature_names[:15]

# Transform x_test using the fitted selector object
x_test_selected = selector.transform(x_test)

# Get the selected feature names for x_train and x_test
selected_train_feature_names = feature_names[selector.get_support()]
selected_test_feature_names = x_test.columns[selector.get_support()]

# Print the most important features for x_train and x_test
print('The 15 most important features in descending order are:', top_feature_names)
print('The selected features for x_train are:', selected_train_feature_names)
print('The selected features for x_test are:', selected_test_feature_names)

"""# Model deplyment and Evaluation

Polynomial Regression
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

# Create a pipeline with PolynomialFeatures and LinearRegression
poly_pipeline = Pipeline([
    ('poly', PolynomialFeatures()),
    ('reg', LinearRegression())
])

# Define the hyperparameters to tune
poly_hyperparameters = {'poly__degree': [1,2,3],
                        'poly__interaction_only': [True, False],
                        'poly__include_bias': [True, False],
                        'poly__order': ['C', 'F']}

# Create a GridSearchCV object
poly_grid_search = GridSearchCV(poly_pipeline, poly_hyperparameters, cv=3)

# Fit the grid search to the data
poly_grid_search.fit(x_train_selected, y_train)

# Get the best hyperparameters
poly_best_params = poly_grid_search.best_params_
print("Best hyperparameters:", poly_best_params)

# Creating a polynomial regression model with best hyperparameters
poly_reg_model = Pipeline([
    ('poly', PolynomialFeatures()),
    ('reg', LinearRegression())
])

poly_reg_model.set_params(poly__degree=poly_best_params['poly__degree'],
                          poly__interaction_only=poly_best_params['poly__interaction_only'],
                          poly__include_bias=poly_best_params['poly__include_bias'],
                          poly__order=poly_best_params['poly__order'])

poly_reg_model.fit(x_train_selected, y_train)

# Testing how the model performs
poly_reg_y_predicted = poly_reg_model.predict(x_test_selected)
poly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))
poly_reg_mse = mean_squared_error(y_test, poly_reg_y_predicted)
poly_reg_r2 = r2_score(y_test, poly_reg_y_predicted)

print("RMSE: ", poly_reg_rmse)
print("MSE: ", poly_reg_mse)
print("R-squared: ", poly_reg_r2)

"""###Model ploting"""

plt.scatter(y_test, poly_reg_y_predicted)
plt.xlabel("Actual Target Values")
plt.ylabel("Predicted Target Values")
plt.title("Polynomial Regression Model")
plt.show()

"""Linear Regression"""

from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression

# Define the hyperparameters to tune
hyperparameters = {'fit_intercept': [True, False],
                   'copy_X': [True, False],
                   'n_jobs': [-1, None],
                   'positive': [True, False]}

# Create an instance of the LinearRegression class
reg = LinearRegression()

# Create a GridSearchCV object
grid_search = GridSearchCV(reg, hyperparameters, cv=5)

# Fit the grid search to the data
grid_search.fit(x_train_selected, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best hyperparameters:", best_params)

# Use the best hyperparameters to fit the model
reg = LinearRegression(**best_params)
reg.fit(x_train_selected, y_train)

# Test the model performance
y_pred = reg.predict(x_test_selected)
mse = mean_squared_error(y_test, y_pred)
reg_r2 = r2_score(y_test, y_pred)

print('Co-efficient of linear regression', reg.coef_)
print('Intercept of linear regression model', reg.intercept_)
print('Mean Square Error:', mse)
print('R-squared :', reg_r2)

"""### Model plotting"""

# Plot predicted values against actual values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Linear Regression Model')
plt.show()

"""Lasso Model"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error

# Define the Lasso model
lasso = Lasso()

# Define the parameter grid to search over
param_grid = {'alpha': [0.01, 0.1, 1, 10]}

# Define the scoring metric to use
scoring_metric = 'neg_mean_squared_error'

# Define the GridSearchCV object
lasso_grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring=scoring_metric, cv=5)

# Fit the GridSearchCV object to the data
lasso_grid_search.fit(x_train_selected, y_train)

# Print the best hyperparameters and best score
print("Best Hyperparameters:", lasso_grid_search.best_params_)
print("Best Score:", -lasso_grid_search.best_score_)

# Make predictions on the test data using the best estimator
y_pred = lasso_grid_search.best_estimator_.predict(x_test_selected)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print('Mean Square Error:', mse)
lasso_r2= r2_score(y_test, y_pred)
print("R-squared :",lasso_r2)

# Print the co-efficients of lasso regression
print('Co-efficient of lasso regression', lasso_grid_search.best_estimator_.coef_)

"""Model plotting"""

plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Lasso Regression Model')
plt.show()

"""Ridge Model"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# Define the Ridge model
ridge = Ridge()

# Define the parameter grid to search over
param_grid = {'alpha': [0.01, 0.1, 1, 10, 50, 100]}

# Define the scoring metric to use
scoring_metric = 'neg_mean_squared_error'

# Define the GridSearchCV object
ridge_grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring=scoring_metric, cv=5)

# Fit the GridSearchCV object to the data
ridge_grid_search.fit(x_train_selected, y_train)

# Print the best hyperparameters and best score
print("Best Hyperparameters:", ridge_grid_search.best_params_)

# Make predictions on the test data using the best estimator
y_pred = ridge_grid_search.best_estimator_.predict(x_test_selected)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
ridge_r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)

print("R-squared:", ridge_r2)

"""Model plotting"""

plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Ridge Regression Model')
plt.show()